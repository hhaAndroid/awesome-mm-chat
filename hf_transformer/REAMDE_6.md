## 6 TOKENIZERS åº“

åœ¨ç¬¬ä¸‰ç« ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•åœ¨ç»™å®šä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥å‘ç° tokenizer æˆ‘ä»¬æ˜¯ç›´æ¥ä½¿ç”¨çš„ï¼Œæ²¡æœ‰è¿›è¡Œè®­ç»ƒï¼Œè¿™ä¼šå­˜åœ¨ä¸è¶³ã€‚å› ä¸ºä½¿ç”¨åœ¨æ¥è‡ªå…¶ä»–é¢†åŸŸæˆ–è¯­è¨€çš„è¯­æ–™åº“ä¸Šé¢„è®­ç»ƒçš„æ ‡è®°å™¨é€šå¸¸ä¸æ˜¯æœ€ç†æƒ³çš„ã€‚ ä¾‹å¦‚ï¼Œåœ¨è‹±è¯­è¯­æ–™åº“ä¸Šè®­ç»ƒçš„æ ‡è®°å™¨åœ¨æ—¥è¯­æ–‡æœ¬è¯­æ–™åº“ä¸Šè¡¨ç°ä¸ä½³ï¼Œå› ä¸ºä¸¤ç§è¯­è¨€ä¸­ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·çš„ä½¿ç”¨éå¸¸ä¸åŒã€‚
å‡è®¾æˆ‘ä»¬è¦å¾®è°ƒçš„è¯­è¨€ä¸å¤ªä¸€æ ·ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±éœ€è¦é‡æ–°è®­ç»ƒ tokenizerï¼Œè¿™å°±æ˜¯ TOKENIZERS åº“çš„ä½œç”¨ã€‚å®ƒæ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒå’Œä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨çš„åº“ï¼Œå®ƒå¯ä»¥è®©æˆ‘ä»¬è½»æ¾åœ°è®­ç»ƒè‡ªå·±çš„åˆ†è¯å™¨ï¼Œç„¶åå°†å…¶ä¸ Hugging Face çš„ Transformer åº“ä¸€èµ·ä½¿ç”¨ã€‚

æ ‡è®°å™¨éœ€è¦ä»”ç»†æŸ¥çœ‹è¯­æ–™åº“ä¸­çš„æ‰€æœ‰æ–‡æœ¬â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸º training çš„è¿‡ç¨‹ã€‚

æ³¨æ„ï¼šè®­ç»ƒæ ‡è®°å™¨ä¸è®­ç»ƒæ¨¡å‹ä¸åŒï¼æ¨¡å‹è®­ç»ƒä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ä½¿æ¯ä¸ªbatchçš„losså°ä¸€ç‚¹ã€‚å®ƒæœ¬è´¨ä¸Šæ˜¯éšæœºçš„ï¼ˆè¿™æ„å‘³ç€åœ¨è¿›è¡Œä¸¤æ¬¡ç›¸åŒçš„è®­ç»ƒæ—¶ï¼Œæ‚¨å¿…é¡»è®¾ç½®ä¸€äº›éšæœºæ•°ç§å­æ‰èƒ½è·å¾—ç›¸åŒçš„ç»“æœï¼‰ã€‚è®­ç»ƒæ ‡è®°å™¨æ˜¯ä¸€ä¸ªç»Ÿè®¡è¿‡ç¨‹ï¼Œå®ƒè¯•å›¾ç¡®å®šå“ªäº›å­è¯æœ€é€‚åˆä¸ºç»™å®šçš„è¯­æ–™åº“é€‰æ‹©ï¼Œç”¨äºé€‰æ‹©å®ƒä»¬çš„ç¡®åˆ‡è§„åˆ™å–å†³äºåˆ†è¯ç®—æ³•ã€‚å®ƒæ˜¯ç¡®å®šæ€§çš„ï¼Œè¿™æ„å‘³ç€åœ¨ç›¸åŒçš„è¯­æ–™åº“ä¸Šä½¿ç”¨ç›¸åŒçš„ç®—æ³•è¿›è¡Œè®­ç»ƒæ—¶ï¼Œæ‚¨æ€»æ˜¯ä¼šå¾—åˆ°ç›¸åŒçš„ç»“æœã€‚

### 1 å¾®è°ƒ tokenizer

å¯ä»¥å…ˆçœ‹ä¸€ä¸ªç®€å•çš„è®­ç»ƒä¾‹å­

```python
import os
# ä¼šè‡ªåŠ¨ä¸‹è½½åˆ°  ~/.cache/huggingface/dataset, æ‚¨å¯ä»¥é€šè¿‡è®¾ç½® HF_HOME ç¯å¢ƒå˜é‡æ¥è‡ªå®šä¹‰ç¼“å­˜çš„æ–‡ä»¶å¤¹
# å¿…é¡»è¦æ”¾åˆ° from datasets import load_dataset å‰é¢ï¼Œå¦åˆ™æ— æ•ˆ
os.environ['HF_HOME'] = '../'  # æ‰€æœ‰ç¼“å­˜éƒ½ä¼šæ”¾åˆ°è¿™ä¸ªè·¯å¾„ä¸‹

from datasets import load_dataset
# This can take a few minutes to load, so grab a coffee or tea while you wait!
raw_datasets = load_dataset("espejelomar/code_search_net_python_10000_examples", "python")
print(raw_datasets["train"])
print(raw_datasets["train"][3456]["whole_func_string"])

def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx: start_idx + 1000]
        yield samples["whole_func_string"]
        
from transformers import AutoTokenizer
old_tokenizer = AutoTokenizer.from_pretrained("gpt2")

example = '''def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
print(len(old_tokenizer))  # 50257

training_corpus = get_training_corpus()
# è®­ç»ƒå‡ºä¸€ä¸ªæ–°çš„ tokenizer
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)

tokens = tokenizer.tokenize(example)
print(len(tokens))  # 27
print(len(old_tokenizer.tokenize(example)))  # 44

print(len(tokenizer))  # 52000
# å¯ä»¥å‘ç°æ€»çš„è¯æ±‡è¡¨å‘ç°äº†å˜åŒ–(ä¿å­˜åˆ°äº† merges.txt é‡Œé¢), finetune çš„æ—¶å€™åº”è¯¥è¦æ”¹è¾“å…¥ embedding å‚æ•°ï¼Œå¦åˆ™å¯èƒ½è¶Šç•Œ, å¯ä»¥çœ‹ demo_2
tokenizer.save_pretrained("code-search-net-tokenizer")
```

è®­ç»ƒå®Œæˆåï¼Œè¯æ±‡è¡¨è‚¯å®šæ˜¯å‘ç”Ÿå˜åŒ–äº†ï¼Œæ­¤æ—¶å¦‚æœè¿æ¥ä¸Šæ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œå¯èƒ½ä¼šå‡ºç°è¶Šç•Œçš„æƒ…å†µï¼Œå› ä¸ºæ¨¡å‹çš„è¾“å…¥ embedding å‚æ•°æ˜¯å›ºå®šçš„ï¼Œæ‰€ä»¥éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚

å¦‚æœä½ æ˜¯ç®€å•çš„æ–°å¢ tokenï¼Œåˆ™æœ‰å¿«æ·åŠæ³•ï¼Œæ— éœ€è®­ç»ƒï¼Œä½†æ˜¯ä¸æ¸…æ¥šä¸Šé¢çš„æƒ…å†µä¸‹æ˜¯å¦ä¸€å®šè¦é‡æ–°è®­ç»ƒ embedding 

```python
import os
os.environ['HF_HOME'] = '../'  # æ‰€æœ‰ç¼“å­˜éƒ½ä¼šæ”¾åˆ°è¿™ä¸ªè·¯å¾„ä¸‹

from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

text = "c_1å››å¤„å¼ æœ›ã€‚"
print(text)  # c_1å››å¤„å¼ æœ›ã€‚
# ['c', '_', '1', 'å››', '[UNK]', '[UNK]', '[UNK]', 'ã€‚']  c_1 ä¼šåˆ‡åˆ†ä¸ºä¸‰ä¸ªå•è¯ï¼Œåç»­è§£ç æ—¶å€™ä¹Ÿæ˜¯å˜æˆä¸‰ä¸ªå•è¯ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå•è¯ï¼Œä¹Ÿå°±æ˜¯è¯´æœ‰ç¼ºé™·
print(tokenizer.tokenize(text))
print(tokenizer.encode(text))
# [CLS] c _ 1 å›› [UNK] [UNK] [UNK] ã€‚ [SEP]
print(tokenizer.decode(tokenizer.encode(text)))

# å¯ä»¥é€šè¿‡æ–°å¢ token æ¥è§£å†³è¿™ä¸ªé—®é¢˜
characters = ["c_1"]
print(tokenizer.vocab_size) # 30522

tokenizer.add_tokens(characters)

print(tokenizer.vocab_size) # 30522
# å¯ä»¥å‘ç°å®é™…ä¸Šæ‰“å°çš„è¯æ±‡è¡¨å¹¶æ²¡æœ‰æ”¹å˜ï¼ŒåŸå› æ˜¯ä»–æ˜¯å•ç‹¬å­˜æ”¾çš„ï¼Œè€Œä¸æ˜¯åˆå¹¶åˆ°è¯æ±‡è¡¨
tokenizer.save_pretrained('aa')

# ä¼šç”Ÿæˆä¸€ä¸ªæ–°çš„ added_tokens.json æ–‡ä»¶
print(len(tokenizer))  # é•¿åº¦+1

# ['c_1', 'å››', '[UNK]', '[UNK]', '[UNK]', 'ã€‚']
print(tokenizer.tokenize(text))
# [CLS] c_1 å›› [UNK] [UNK] [UNK] ã€‚ [SEP]
print(tokenizer.decode(tokenizer.encode(text)))

# è¿™ä¸ªæ­¥éª¤å¿…é¡»ï¼Œå› ä¸ºæ€»çš„è¯æ±‡è¡¨å˜äº†ï¼Œéœ€è¦å¯¹æ–°å¢çš„éƒ¨åˆ†éšæœºåˆå§‹åŒ–
# æ¨¡å‹éœ€è¦è°ƒç”¨ resize_token_embeddingsï¼Œé¢„è®­ç»ƒçš„ Embedding ä¸å˜ï¼Œæ·»åŠ çš„ token éšæœºåˆå§‹åŒ–è¿› Embedding çŸ©é˜µä¸­ã€‚
# å‡è®¾åŸå…ˆè¯æ±‡è¡¨å¤§å°æ˜¯ 100ï¼Œç»´åº¦æ˜¯ 512,é‚£ä¹ˆåŸå…ˆçš„ embeding ç»´åº¦å°±æ˜¯ 100x512, ç°åœ¨å¢åŠ äº† 10 ä¸ª tokenï¼Œé‚£ä¹ˆæ–°çš„ embeding ç»´åº¦å°±æ˜¯ 110x512
# ä½†æ˜¯åŸå…ˆçš„ 100 ä¸ª token çš„ embeding æ˜¯é¢„è®­ç»ƒå¥½çš„ï¼Œæ‰€ä»¥ä¸éœ€è¦æ”¹å˜ï¼Œåªéœ€è¦æŠŠæ–°å¢çš„ 10 ä¸ª token çš„ embeding éšæœºåˆå§‹åŒ–è¿›å»å°±å¯ä»¥äº†
model = BertModel.from_pretrained("bert-base-uncased")
model.resize_token_embeddings(len(tokenizer))
```

`resize_token_embeddings` çš„è¿‡ç¨‹éå¸¸æš´åŠ›ï¼Œå°±æ˜¯ç›´æ¥ copy+å‰©ä¸‹çš„éšæœºåˆå§‹åŒ–ã€‚

### 2 

æ…¢é€Ÿåˆ†è¯å™¨æ˜¯åœ¨ ğŸ¤— Transformers åº“ä¸­ç”¨ Python ç¼–å†™çš„ï¼Œè€Œå¿«é€Ÿç‰ˆæœ¬æ˜¯ç”± ğŸ¤— åˆ†è¯å™¨æä¾›çš„ï¼Œå®ƒä»¬æ˜¯ç”¨ Rust ç¼–å†™çš„ã€‚

